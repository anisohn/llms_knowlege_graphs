```markdown
# üìö Apprentissage Personnalis√© avec LLMs : Combinaison de Graphes de Connaissances et Datasets  

Ce projet vise √† exploiter la puissance des **graphes de connaissances** et des **datasets d'apprentissage** pour cr√©er un mod√®le de **LLM personnalis√©**, capable de g√©n√©rer des explications adapt√©es en fonction du niveau et des lacunes d'un √©l√®ve.  

## üöÄ Objectif

Cr√©er un syst√®me interactif d'apprentissage qui :
- Analyse des documents PDF pour en extraire les connaissances
- G√©n√®re des interactions p√©dagogiques personnalis√©es
- Maintient un contexte conversationnel avec m√©moire
- Produit des exercices et quiz adaptatifs

## ‚ú® Fonctionnalit√©s Cl√©s

- **Analyse de documents PDF** avec extraction de texte et d√©coupage intelligent
- **Chat interactif** avec m√©moire de conversation contextuelle
- **G√©n√©ration automatique** :
  - Exemples concrets bas√©s sur le document
  - Quiz personnalis√©s avec questions vari√©es
  - Suggestions de questions d'approfondissement
- **Recherche s√©mantique** dans les documents avec ChromaDB
- **Int√©gration de mod√®les LLM locaux** via Ollama

## üõ† Technologies Utilis√©es

- **Ollama** : Interface locale pour l'ex√©cution de LLMs (Mistral)
- **LangChain** : Orchestration des flux de traitement NLP
- **ChromaDB** : Base de donn√©es vectorielle pour la recherche s√©mantique
- **Chainlit** : Interface utilisateur conversationnelle
- **PyPDF2** : Extraction de contenu depuis fichiers PDF
- **RecursiveCharacterTextSplitter** : D√©coupage contextuel du texte

## üì¶ Installation

1. Pr√©requis :
```bash
ollama pull mistral
ollama pull nomic-embed-text
```

2. Installer les d√©pendances :
```bash
pip install chainlit langchain langchain-community langchain-chroma pypdf2 python-dotenv ollama
```

## üñ• Utilisation

1. D√©marrer l'application :
```bash
chainlit run votre_fichier.py -w
```

2. Workflow :
- Upload d'un document PDF
- Interaction naturelle via :
  - Questions directes sur le contenu
  - Boutons d'actions pr√©d√©finies :
    - G√©n√©rer des exemples pratiques
    - Cr√©er un quiz interactif
    - Obtenir des questions d'approfondissement
- R√©ponses contextuelles avec sources documentaires

## ‚öôÔ∏è Customisation

Modifier dans le code :
```python
# Mod√®le LLM
llm = Ollama(model="mistral")  # Changer le mod√®le

# Param√®tres de d√©coupage
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,  # Taille des segments textuels
    chunk_overlap=50   # Chevauchement contextuel
)

# Embeddings
embeddings = OllamaEmbeddings(model="nomic-embed-text")  # Mod√®le d'embedding
```



## ‚ö†Ô∏è Important

L'application n√©cessite :
- Ollama install√© localement
- Les mod√®les Mistral et nomic-embed-text t√©l√©charg√©s
- Une connexion internet pour le premier lancement (t√©l√©chargement des mod√®les)
```
