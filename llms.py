import chainlit as cl
import PyPDF2
from pdf2image import convert_from_path
import pytesseract
import random

from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory, ChatMessageHistory
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import Ollama  # Ou ton mod√®le LLM pr√©f√©r√©
from chainlit.input_widget import AskFileMessage

llm = Ollama(model="llama3")  # Utilise ton mod√®le LLM ici

def extract_text_from_pdf(file_path):
    pdf = PyPDF2.PdfReader(file_path)
    pdf_text = ""
    images = convert_from_path(file_path)
    ocr_text = ""

    for page_number, page in enumerate(pdf.pages):
        text = page.extract_text()
        if text:
            pdf_text += text
        else:
            print(f"Pas de texte trouv√© sur la page {page_number + 1}, tentative d'OCR.")
            img = images[page_number]
            ocr_text += pytesseract.image_to_string(img)

    full_text = pdf_text + "\n" + ocr_text
    return None if not full_text.strip() else full_text

@cl.on_chat_start
async def on_chat_start():
    files = None
    while files is None:
        files = await cl.AskFileMessage(
            content="üìÑ Veuillez uploader un fichier PDF pour commencer !",
            accept=["application/pdf"],
            max_size_mb=100,
            timeout=180
        ).send()

    file = files[0]
    pdf_text = extract_text_from_pdf(file.path)

    if pdf_text is None:
        await cl.Message(content="‚ùå Le PDF ne contient pas de texte d√©tectable. Essayez un autre fichier.").send()
        return

    cl.user_session.set("full_pdf_text", pdf_text)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=50
    )
    texts = text_splitter.split_text(pdf_text)
    metadatas = [{"source": f"{i}-pl"} for i in range(len(texts))]

    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    docsearch = await cl.make_async(Chroma.from_texts)(
        texts, embeddings, metadatas=metadatas
    )

    message_history = ChatMessageHistory()
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        output_key="answer",
        chat_memory=message_history,
        return_messages=True,
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=docsearch.as_retriever(),
        memory=memory,
        return_source_documents=True
    )

    cl.user_session.set("chain", chain)

    await cl.Message(content=f"‚úÖ Le fichier `{file.name}` a √©t√© trait√© avec succ√®s !\n\nüîÑ G√©n√©ration automatique des contenus en cours...").send()

    await auto_generate_all(chain, pdf_text)

async def auto_generate_all(chain, text):
    seed = random.randint(1, 100)

    # G√©n√©rer un quiz
    quiz_prompt = (
        f"Using seed {seed}, generate exactly 5 quiz questions. Each should have one correct answer, "
        f"three incorrect alternatives, and an explanation for the correct choice.\n\n{text}"
    )
    quiz_res = await chain.acall({"question": quiz_prompt}, callbacks=[cl.AsyncLangchainCallbackHandler()])
    await cl.Message(content="üìò **Quiz g√©n√©r√© :**\n\n" + quiz_res["answer"]).send()

    # G√©n√©rer des exemples
    examples_prompt = f"Using seed {seed}, generate exactly 5 creative examples that illustrate key concepts from the text.\n\n{text}"
    examples_res = await chain.acall({"question": examples_prompt}, callbacks=[cl.AsyncLangchainCallbackHandler()])
    await cl.Message(content="üí° **Exemples g√©n√©r√©s :**\n\n" + examples_res["answer"]).send()

    # G√©n√©rer des questions de r√©flexion
    themes = ["Factual Details", "Interpretative Insights", "Critical Evaluations"]
    theme = random.choice(themes)
    questions_prompt = f"Using seed {seed}, generate 5 unique questions focusing on '{theme}'.\n\n{text}"
    questions_res = await chain.acall({"question": questions_prompt}, callbacks=[cl.AsyncLangchainCallbackHandler()])
    await cl.Message(content=f"‚ùì **Questions de r√©flexion ({theme}) :**\n\n" + questions_res["answer"]).send()

    # G√©n√©rer une explication p√©dagogique
    explanation_prompt = "Provide a detailed and pedagogical explanation of a key concept from the text.\n\n" + text
    explanation_res = await chain.acall({"question": explanation_prompt}, callbacks=[cl.AsyncLangchainCallbackHandler()])
    sources = "\n\nüìé **Sources :**\n" + "\n".join([f"- {doc.metadata['source']}" for doc in explanation_res["source_documents"]])
    await cl.Message(content="üìö **Explication p√©dagogique :**\n\n" + explanation_res["answer"] + sources).send()

@cl.on_message
async def main(message: cl.Message):
    chain = cl.user_session.get("chain")
    cb = cl.AsyncLangchainCallbackHandler()

    res = await chain.acall({"question": message.content}, callbacks=[cb])
    answer = res["answer"]
    sources = res.get("source_documents", [])

    sources_content = "\n\nüìé **Sources :**\n" + "\n".join([f"- {doc.metadata['source']}" for doc in sources]) if sources else ""
    await cl.Message(content=answer + sources_content).send()
